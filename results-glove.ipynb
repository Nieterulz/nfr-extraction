{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results-glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rsQKXx3-HtU"
      },
      "source": [
        "# Resultados GloVe\n",
        "En este notebook nos encargamos de entrenar los modelos y mostrar resultados para cada uno de ellos. Mostraremos precisión, sensibilidad y F1 para cada categoría así como la media que estos ofrecen. Como técnica de conversión a formato numérico utilizaremos GloVe.\n",
        "\n",
        "### Activamos Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECe6jSgYIXaw",
        "outputId": "5b22b1db-a561-4d7a-b20c-3fe7d9631193"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwHPcePG__B"
      },
      "source": [
        "### Importamos librerías necesarias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4XXL0ROU9C6",
        "outputId": "49b1171d-2b47-4d0a-94ef-decc251f5f79"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm, naive_bayes\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from joblib import dump, load\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxlDaxe35Dr5"
      },
      "source": [
        "### Definimos función que nos dice si una palabra es un adjetivo, un nombre, un verbo o un advervio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aR5lbJPTvHd"
      },
      "source": [
        "def get_wordnet_pos(word):\r\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\r\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\r\n",
        "                \"N\": wordnet.NOUN,\r\n",
        "                \"V\": wordnet.VERB,\r\n",
        "                \"R\": wordnet.ADV}\r\n",
        "\r\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-uQDOuQTfB"
      },
      "source": [
        "### Definimos función que recibe una cadena de texto y devuelve el texto procesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqHYKmrSVIbR"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    # Convertimos a minúsculas\n",
        "    new_text = sentence.lower()\n",
        "    \n",
        "    # Eliminamos puntuación\n",
        "    new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
        "\n",
        "    # Dividimos en tokens\n",
        "    tokens = nltk.tokenize.TreebankWordTokenizer().tokenize(new_text)\n",
        "\n",
        "    # Eliminamos stopwords\n",
        "    tokens = [word for word in tokens if not word in nltk.corpus.stopwords.words('english')]\n",
        "    new_text = ' '.join(word for word in tokens)\n",
        "\n",
        "    # Stemming\n",
        "    # stemmer = nltk.stem.PorterStemmer()\n",
        "    # new_text = ' '.join([stemmer.stem(w) for w in tokens])\n",
        "    \n",
        "    # lemma\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    new_text = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens])\n",
        "\n",
        "    # Reemplazamos números por #s\n",
        "    if bool(re.search(r'\\d', new_text)):\n",
        "        new_text = re.sub('[0-9]{5,}', '#####', new_text)\n",
        "        new_text = re.sub('[0-9]{4}', '####', new_text)\n",
        "        new_text = re.sub('[0-9]{3}', '###', new_text)\n",
        "        new_text = re.sub('[0-9]{2}', '##', new_text)\n",
        "        # Cuando existe un solo número lo eliminamos\n",
        "        new_text = re.sub('[0-9]{1}', '', new_text)\n",
        "\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asqPLsGK8qpv"
      },
      "source": [
        "###Definimos función que calcula la media armónica de 2 números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rhVLfWV8o8x"
      },
      "source": [
        "def calculate_f1_score(precision, recall):\r\n",
        "    return 2*((precision * recall) / (precision + recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVNfQm16AHl"
      },
      "source": [
        "### Seleccionamos el modelo que queremos utilizar. Por defecto se utilizará k-NN con k=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e-rg-Od6IKJ"
      },
      "source": [
        "model_list = [\n",
        "    ('K-NN con k=1', 0), \n",
        "    ('SVM núcleo lineal', 1), \n",
        "    ('Clasificador bayesiano ingenuo', 2), \n",
        "    ('Bosque aleatorio', 3)\n",
        "]\n",
        "model_picker = widgets.Dropdown(options=model_list)\n",
        "print(\"Selecciona un modelo: \")\n",
        "model_picker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyUvuBFoT0GB"
      },
      "source": [
        "### Cargamos el modelo GloVe para poder realizar las conversiones de cadena de texto a vector de números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un4UnjLoVXCR"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7vQn1pQA_Xx"
      },
      "source": [
        "### Cargamos y preprocesamos la base de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2R9TP54T8xt"
      },
      "source": [
        "RUTA_DB = \"./drive/My Drive/xlsx/db.xlsx\"\r\n",
        "db = pd.read_excel(RUTA_DB)\r\n",
        "categories = [\r\n",
        "        \"access control\", \"audit\", \"availability\", \"legal\", \"look and feel\",\r\n",
        "        \"maintainability\", \"operational\", \"privacy\", \"recoverability\", \"capacity and performance\",\r\n",
        "        \"reliability\", \"security\", \"usability\", \"other nonfunctional\", \"functional\", \"not applicable\"\r\n",
        "    ]\r\n",
        "\r\n",
        "# Creamos diccionario con las traducciones para cada categoría\r\n",
        "cat_translations =[\r\n",
        "    \"Control de acceso\", \"Auditoría\", \"Disponibilidad\", \"Legal\", \"Diseño\",\r\n",
        "    \"Mantenibilidad\", \"Operacional\", \"Privacidad\", \"Recuperabilidad\", \"Rendimiento\",\r\n",
        "    \"Fiabilidad\", \"Seguridad\", \"Usabilidad\", \"Otros no funcionales\", \"Funcional\", \"No aplicable\"    \r\n",
        "]\r\n",
        "\r\n",
        "db['sentences'] = db['sentences'].apply(preprocess)\r\n",
        "db = db.dropna()\r\n",
        "db = db.sample(frac=1, random_state = 5).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUDuNECRUKBT"
      },
      "source": [
        "### Dividimos los datos en datos de entrada y clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIkHNEV46o1X"
      },
      "source": [
        "x = normalize([nlp(s).vector for s in db['sentences'].values])\r\n",
        "y = db.drop(labels=['sentences'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToMTS1n9Qn1m"
      },
      "source": [
        "### Entrenamos modelo, validamos y mostramos resultados de precisión, sensibilidad y F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG1lRWYQAB92"
      },
      "source": [
        "# Suprimimos posibles alertas\r\n",
        "warnings.filterwarnings(\"ignore\") \r\n",
        "\r\n",
        "# Definimos lista con los posibles modelos\r\n",
        "models = [KNeighborsClassifier(n_neighbors=1), svm.LinearSVC(), naive_bayes.BernoulliNB(), RandomForestClassifier(n_jobs=-1)]\r\n",
        "# Escogemos el modelo que vamos a utilizar\r\n",
        "model = models[model_picker.value]\r\n",
        "\r\n",
        "# Definimos variables necesarias para el almacenamiento de los resultados\r\n",
        "n_splits = 10\r\n",
        "mean_precision = [[0]*16]*n_splits\r\n",
        "mean_recall = [[0]*16]*n_splits\r\n",
        "mean_f1score = [[0]*16]*n_splits\r\n",
        "i_test = 0 \r\n",
        "f_test = math.floor(len(db)/n_splits)\r\n",
        "\r\n",
        "# Iteramos n_splits veces (Validación cruzada con n_splits = 10)\r\n",
        "for i_split in range(n_splits):\r\n",
        "    print(\"\\nIteración\", i_split+1, end='')\r\n",
        "    # Definimos variables de precisión, sensibilidad y F1 para esta iteración \r\n",
        "    precision = [0]*len(categories)\r\n",
        "    recall = [0]*len(categories)\r\n",
        "    f1score = [0]*len(categories)\r\n",
        "\r\n",
        "    # Dividimos en vectores de características de entrenamiento y validación,\r\n",
        "    # y clasificaciones para entrenamiento y validaciób\r\n",
        "    x_train = list(x)[:i_test] + list(x)[f_test:]\r\n",
        "    y_train = y.drop(range(i_test, f_test))\r\n",
        "    x_test = list(x)[i_test:f_test]\r\n",
        "    y_test = y[i_test:f_test]\r\n",
        "\r\n",
        "    # Para cada categoría\r\n",
        "    for i, category in enumerate(categories):\r\n",
        "        print('.', end='')\r\n",
        "        # Entrenamos el modelo\r\n",
        "        model.fit(x_train, y_train[category])\r\n",
        "        # Realizamos predicción para conjunto de prueba\r\n",
        "        prediction = model.predict(x_test)\r\n",
        "        # Obtenemos resultados\r\n",
        "        precision[i] = precision_score(list(y_test[category].values), prediction)\r\n",
        "        recall[i] = recall_score(list(y_test[category].values), prediction)\r\n",
        "\r\n",
        "    i_test = f_test\r\n",
        "    f_test = i_test + math.floor(len(db)/n_splits)\r\n",
        "\r\n",
        "    mean_precision[i_split] = precision\r\n",
        "    mean_recall[i_split] = recall\r\n",
        "    mean_f1score[i_split] = f1score\r\n",
        "\r\n",
        "# Mostramos resultados agrupados en un dataframe de pandas\r\n",
        "p = list(np.average(mean_precision, axis=0))\r\n",
        "rounded_p = [round(elem, 2) for elem in p]\r\n",
        "r = list(np.average(mean_recall, axis=0))\r\n",
        "rounded_r = [round(elem, 2) for elem in r]\r\n",
        "rounded_f1 = [\r\n",
        "    round(calculate_f1_score(rounded_p[i], rounded_r[i]), 2) for i, elem in enumerate(rounded_p)\r\n",
        "]\r\n",
        "data = {\r\n",
        "    \" \": cat_translations + ['MICRO AVG'],\r\n",
        "    \"Precisión\": rounded_p + [np.average(rounded_p)],\r\n",
        "    \"Sensibilidad\": rounded_r + [np.average(rounded_r)],\r\n",
        "    \"F1\": rounded_f1 + [calculate_f1_score(np.average(rounded_p), np.average(rounded_r))]\r\n",
        "}\r\n",
        "print(\"\\nLos resultados para el modelo seleccionado son:\")\r\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\r\n",
        "pd.DataFrame(data).set_index(' ')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-0oM1GGSAs"
      },
      "source": [
        "### Exportamos los modelos\n",
        "Los modelos que exportaremos coincidiran con los entrenados en la primera\n",
        "iteración de la validación cruzada. De esta manera podremos utilizar estos modelos en otro lugar para poder clasificar nuevos elementos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k77baK6Iz1a"
      },
      "source": [
        "i_test = 0\n",
        "f_test = math.floor(len(db)/n_splits)\n",
        "\n",
        "x_train = list(x)[:i_test] + list(x)[f_test:]\n",
        "y_train = y.drop(range(i_test, f_test))\n",
        "x_test = list(x)[i_test:f_test]\n",
        "y_test = y[i_test:f_test]\n",
        "\n",
        "# Para cada categoría\n",
        "for category in categories[:-1]:\n",
        "    model = models[model_picker.value]\n",
        "    model.fit(x_train, y_train[category])\n",
        "    dump(model, './drive/My Drive/nfr-extraction/models-glove/' + category.replace(\" \", \"\") + '.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}