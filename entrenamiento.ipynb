{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entrenamiento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwHPcePG__B"
      },
      "source": [
        "#Importamos librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLdx4qGNpt4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331f4bfd-d841-49b5-a06c-c234b5dbeb82"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm, naive_bayes\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1THr93UHHK7"
      },
      "source": [
        "#Cargamos la base de datos y definimos las categorías existentes en ella"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-uQDOuQTfB"
      },
      "source": [
        "# Definimos función que preprocesa la base de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXmtBMfqPKh"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    # elimina puntuación\n",
        "    new_text = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    # elimina números\n",
        "    new_text = re.sub(r'\\d+','',new_text) \n",
        "    # minúsculas\n",
        "    new_text = new_text.lower() \n",
        "    # dividimos en tokens\n",
        "    tokens = nltk.tokenize.word_tokenize(new_text)\n",
        "    # eliminamos stopwords\n",
        "    tokens = [word for word in tokens if not word in nltk.corpus.stopwords.words('english')]\n",
        "    # lemmatizamos\n",
        "    # lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    # new_text = ' '.join([lemmatizer.lemmatize(w) for w in tokens])\n",
        "    # stemming\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    new_text = ' '.join([stemmer.stem(w) for w in tokens])\n",
        "    \n",
        "    # new_text = ' '.join(tokens)\n",
        "\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_d1MNwLQbnJ"
      },
      "source": [
        "# Definimos función que transforma el texto preprocesado en un vector de números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8-SMYxLQjlz"
      },
      "source": [
        "def useTfIdf(db, i_test, f_test):\n",
        "    train = db.drop(range(i_test, f_test))\n",
        "    test = db[i_test:f_test]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    vectorizer.fit(list(train['sentences'].values))\n",
        "\n",
        "    x_train = vectorizer.transform(list(train['sentences'].values))\n",
        "    y_train = train.drop(labels=['sentences'], axis=1)\n",
        "    x_test = vectorizer.transform(list(test['sentences'].values))\n",
        "    y_test = test.drop(labels=['sentences'], axis=1)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToMTS1n9Qn1m"
      },
      "source": [
        "# Entrenamos modelo, validamos y mostramos resultados de precisión, sensibilidad y F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIber3M3vASL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d01104-2e8f-4f3c-9304-3240950e82c8"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "RUTA_DB = \"./drive/My Drive/xlsx/db.xlsx\"\n",
        "db = pd.read_excel(RUTA_DB)\n",
        "categories = [\n",
        "        \"access control\", \"audit\", \"availability\", \"legal\", \"look and feel\",\n",
        "        \"maintainability\", \"operational\", \"privacy\", \"recoverability\", \"capacity and performance\",\n",
        "        \"reliability\", \"security\", \"usability\", \"other nonfunctional\", \"functional\", \"not applicable\"\n",
        "    ]\n",
        "\n",
        "k = 10\n",
        "db['sentences'] = db['sentences'].apply(preprocess)\n",
        "db = db.dropna()\n",
        "\n",
        "mean_precision = [[0]*16]*k\n",
        "mean_recall = [[0]*16]*k\n",
        "mean_f1score = [[0]*16]*k\n",
        "\n",
        "for iter in range(iterations):\n",
        "    precision = [0]*len(categories)\n",
        "    recall = [0]*len(categories)\n",
        "    f1score = [0]*len(categories)\n",
        "\n",
        "    db = db.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    i_test = 0 \n",
        "    f_test = math.floor(len(db)/k)\n",
        "\n",
        "\n",
        "    for index_k in range(k):\n",
        "        precision = [0]*len(categories)\n",
        "        recall = [0]*len(categories)\n",
        "        f1score = [0]*len(categories)\n",
        "        x_train, y_train, x_test, y_test = useTfIdf(db, i_test, f_test)\n",
        "\n",
        "        # model = KNeighborsClassifier(n_neighbors=1)\n",
        "        model = svm.LinearSVC()\n",
        "        # model = naive_bayes.MultinomialNB()\n",
        "        # model = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "        i = 0\n",
        "        for category in categories:\n",
        "            model.fit(x_train, y_train[category])\n",
        "            prediction = model.predict(x_test)\n",
        "            precision[i] += precision_score(y_test[category], prediction)\n",
        "            recall[i] += recall_score(y_test[category], prediction)\n",
        "            f1score[i] += f1_score(y_test[category], prediction)\n",
        "            i += 1\n",
        "        \n",
        "        i_test = f_test\n",
        "        f_test = i_test + math.floor(len(db)/k)\n",
        "        \n",
        "        mean_precision[index_k] = precision\n",
        "        mean_recall[index_k] = recall\n",
        "        mean_f1score[index_k] = f1score\n",
        "\n",
        "print(\"Precisión\")\n",
        "print(np.average(mean_precision, axis=0))\n",
        "\n",
        "print(\"\\nSensibilidad\")\n",
        "print(np.average(mean_recall, axis=0))\n",
        "\n",
        "print(\"\\nF1\")\n",
        "print(np.average(mean_f1score, axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision\n",
            "[0.79861144 0.79543088 0.63333333 0.86223332 0.90833333 0.79518673\n",
            " 0.72585082 0.81060593 0.69166667 0.90321429 0.25       0.79188775\n",
            " 0.93571429 0.31666667 0.8718735  0.91403368]\n",
            "\n",
            "Recall\n",
            "[0.61873664 0.46420126 0.2424359  0.45668281 0.30889971 0.450104\n",
            " 0.34703541 0.4524161  0.30095238 0.57819112 0.10333333 0.51603175\n",
            " 0.40612554 0.06746753 0.83735831 0.92396528]\n",
            "\n",
            "F1 Score\n",
            "[0.6955894  0.58225447 0.33864613 0.58114454 0.44056999 0.57307822\n",
            " 0.4562281  0.57358279 0.38621212 0.68643978 0.14       0.62289839\n",
            " 0.56196018 0.10621212 0.85416724 0.91891778]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}