{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd7a9572091a45d7bd2c25c0b25cf32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "SVM núcleo lineal",
              "K-NN con k=1",
              "Clasificador bayesiano ingenuo",
              "Bosque aleatorio"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_ef87a99afafa42719630b36a5e6f9aa8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f36a91711b5143f6b0cc3f113f443b7a"
          }
        },
        "ef87a99afafa42719630b36a5e6f9aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f36a91711b5143f6b0cc3f113f443b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rsQKXx3-HtU"
      },
      "source": [
        "# Resultados\n",
        "En este notebook nos encargamos de entrenar los modelos y mostrar resultados para cada uno de ellos. Mostraremos precisión, sensibilidad y F1 para cada categoría así como la media que estos ofrecen.\n",
        "\n",
        "### Activamos Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECe6jSgYIXaw",
        "outputId": "826b5d17-7559-45e0-9402-febbaa282d1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwHPcePG__B"
      },
      "source": [
        "### Importamos librerías necesarias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLdx4qGNpt4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c84e244-19bb-4849-e4a7-fb511bfae148"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import warnings\n",
        "import ipywidgets as widgets\n",
        "from joblib import dump\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm, naive_bayes\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxlDaxe35Dr5"
      },
      "source": [
        "### Definimos función que nos dice si una palabra es un adjetivo, un nombre, un verbo o un advervio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNU5Ovw75Kpg"
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-uQDOuQTfB"
      },
      "source": [
        "### Definimos función que recibe una cadena de texto y devuelve el texto procesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXmtBMfqPKh"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    # Convertimos a minúsculas\n",
        "    new_text = sentence.lower()\n",
        "    \n",
        "    # Eliminamos puntuación\n",
        "    new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
        "\n",
        "    # Dividimos en tokens\n",
        "    tokens = nltk.tokenize.TreebankWordTokenizer().tokenize(new_text)\n",
        "\n",
        "    # Eliminamos stopwords\n",
        "    tokens = [word for word in tokens if not word in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "    # Stemming\n",
        "    # stemmer = nltk.stem.PorterStemmer()\n",
        "    # new_text = ' '.join([stemmer.stem(w) for w in tokens])\n",
        "    \n",
        "    # lemma\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    new_text = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens])\n",
        "\n",
        "    # new_text = ' '.join([w for w in tokens])\n",
        "\n",
        "    # Reemplazamos números por #s\n",
        "    if bool(re.search(r'\\d', new_text)):\n",
        "        new_text = re.sub('[0-9]{5,}', '#####', new_text)\n",
        "        new_text = re.sub('[0-9]{4}', '####', new_text)\n",
        "        new_text = re.sub('[0-9]{3}', '###', new_text)\n",
        "        new_text = re.sub('[0-9]{2}', '##', new_text)\n",
        "        # Cuando existe un solo número lo eliminamos\n",
        "        new_text = re.sub('[0-9]{1}', '', new_text)\n",
        "\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVNfQm16AHl"
      },
      "source": [
        "### Seleccionamos el modelo que queremos utilizar. Por defecto se utilizará SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "fd7a9572091a45d7bd2c25c0b25cf32f",
            "ef87a99afafa42719630b36a5e6f9aa8",
            "f36a91711b5143f6b0cc3f113f443b7a"
          ]
        },
        "id": "6e-rg-Od6IKJ",
        "outputId": "7a77821e-22d9-4845-8cf5-a8b9a0e68040"
      },
      "source": [
        "model_list = [('SVM núcleo lineal', 0), ('K-NN con k=1', 1), ('Clasificador bayesiano ingenuo', 2), ('Bosque aleatorio', 3)]\n",
        "model_picker = widgets.Dropdown(options=model_list)\n",
        "print(\"Selecciona un modelo: \")\n",
        "model_picker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecciona un modelo: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd7a9572091a45d7bd2c25c0b25cf32f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dropdown(options=(('SVM núcleo lineal', 0), ('K-NN con k=1', 1), ('Clasificador bayesiano ingenuo', 2), ('Bosq…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7vQn1pQA_Xx"
      },
      "source": [
        "### Cargamos y preprocesamos la base de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OPFzy0jBAj5"
      },
      "source": [
        "# Cargamos base de datos\n",
        "RUTA_DB = \"./drive/My Drive/nfr-extraction/db.xlsx\"\n",
        "db = pd.read_excel(RUTA_DB)\n",
        "\n",
        "# Definimos categorías existentes\n",
        "categories = [\n",
        "    \"access control\", \"audit\", \"availability\", \"legal\", \"look and feel\",\n",
        "    \"maintainability\", \"operational\", \"privacy\", \"recoverability\", \"capacity and performance\",\n",
        "    \"reliability\", \"security\", \"usability\", \"other nonfunctional\", \"functional\", \"not applicable\"\n",
        "]\n",
        "\n",
        "# Preprocesamos los elementos de la base de datos y barajamos de forma aleatoria\n",
        "db['sentences'] = db['sentences'].apply(preprocess)\n",
        "db = db.dropna()\n",
        "\n",
        "db = db.sample(frac=1, random_state = 42).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToMTS1n9Qn1m"
      },
      "source": [
        "### Entrenamos modelo, validamos y mostramos resultados de precisión, sensibilidad y F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIber3M3vASL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "ace132ed-ccd2-43b6-f679-508eb6103bbe"
      },
      "source": [
        "# Suprimimos posibles alertas\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "# Definimos lista con los posibles modelos\n",
        "models = [svm.LinearSVC(), KNeighborsClassifier(n_neighbors=1), naive_bayes.MultinomialNB(), RandomForestClassifier(n_jobs=-1)]\n",
        "# Escogemos el modelo que vamos a utilizar\n",
        "model = models[model_picker.value]\n",
        "\n",
        "# Definimos variables necesarias para el almacenamiento de los resultados\n",
        "k = 10\n",
        "mean_precision = [[0]*16]*k\n",
        "mean_recall = [[0]*16]*k\n",
        "mean_f1score = [[0]*16]*k\n",
        "i_test = 0 \n",
        "f_test = math.floor(len(db)/k)\n",
        "\n",
        "# Iteramos k veces (Validación cruzada con k = 10)\n",
        "for index_k in range(k):\n",
        "    print(\"\\nIteración\", index_k+1, end='')\n",
        "    # Definimos variables de precisión, sensibilidad y F1 para esta iteración \n",
        "    precision = [0]*len(categories)\n",
        "    recall = [0]*len(categories)\n",
        "    f1score = [0]*len(categories)\n",
        "\n",
        "    # Dividimos en datos de entrenamiento y validación\n",
        "    train = db.drop(range(i_test, f_test))\n",
        "    test = db[i_test:f_test]\n",
        "\n",
        "    # Instanciamos el modelo que se encargará de aplicar tfidf a las frases\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Lo entrenamos con el conjunto de datos de entrenamiento\n",
        "    vectorizer.fit(list(train['sentences'].values))\n",
        "\n",
        "    # Dividimos en vectores de características de entrenamiento y validación,\n",
        "    # y clasificaciones para entrenamiento y validaciób\n",
        "    x_train = vectorizer.transform(list(train['sentences'].values))\n",
        "    y_train = train.drop(labels=['sentences'], axis=1)\n",
        "    x_test = vectorizer.transform(list(test['sentences'].values))\n",
        "    y_test = test.drop(labels=['sentences'], axis=1)\n",
        "\n",
        "    i = 0\n",
        "    # Para cada categoría\n",
        "    for category in categories:\n",
        "        print('.', end='.')\n",
        "        # Entrenamos el modelo\n",
        "        model.fit(x_train, y_train[category])\n",
        "        # Realizamos predicción para conjunto de prueba\n",
        "        prediction = model.predict(x_test)\n",
        "        # Obtenemos resultados\n",
        "        precision[i] += precision_score(list(y_test[category].values), prediction)\n",
        "        recall[i] += recall_score(list(y_test[category].values), prediction)\n",
        "        f1score[i] += f1_score(list(y_test[category].values), prediction)\n",
        "        i += 1\n",
        "    \n",
        "    i_test = f_test\n",
        "    f_test = i_test + math.floor(len(db)/k)\n",
        "    \n",
        "    mean_precision[index_k] = precision\n",
        "    mean_recall[index_k] = recall\n",
        "    mean_f1score[index_k] = f1score\n",
        "\n",
        "# Mostramos resultados agrupados en un dataframe de pandas\n",
        "p = list(np.average(mean_precision, axis=0))\n",
        "rounded_p = [round(elem, 2) for elem in p]\n",
        "r = list(np.average(mean_recall, axis=0))\n",
        "rounded_r = [round(elem, 2) for elem in r]\n",
        "f1 = list(np.average(mean_f1score, axis=0))\n",
        "rounded_f1 = [round(elem, 2) for elem in f1]\n",
        "data = {\n",
        "    \" \": categories + ['MICRO AVG'],\n",
        "    \"Precision\": rounded_p + [np.average(rounded_p)],\n",
        "    \"Recall\": rounded_r + [np.average(rounded_r)],\n",
        "    \"F1\": rounded_f1 + [np.average(rounded_f1)]\n",
        "}\n",
        "print(\"\\nLos resultados para el modelo seleccionado son:\")\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "pd.DataFrame(data).set_index(' ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteración 1................................\n",
            "Iteración 2................................\n",
            "Iteración 3................................\n",
            "Iteración 4................................\n",
            "Iteración 5................................\n",
            "Iteración 6................................\n",
            "Iteración 7................................\n",
            "Iteración 8................................\n",
            "Iteración 9................................\n",
            "Iteración 10................................\n",
            "Los resultados para el modelo seleccionado son:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>access control</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>audit</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>availability</th>\n",
              "      <td>0.62</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>legal</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>look and feel</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maintainability</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>operational</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>privacy</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recoverability</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>capacity and performance</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reliability</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>security</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>usability</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other nonfunctional</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>functional</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not applicable</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO AVG</th>\n",
              "      <td>0.77</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          Precision  Recall   F1\n",
              "                                                \n",
              "access control                 0.80    0.64 0.71\n",
              "audit                          0.83    0.48 0.61\n",
              "availability                   0.62    0.26 0.36\n",
              "legal                          0.85    0.44 0.58\n",
              "look and feel                  0.87    0.34 0.47\n",
              "maintainability                0.87    0.48 0.61\n",
              "operational                    0.77    0.38 0.50\n",
              "privacy                        0.82    0.47 0.60\n",
              "recoverability                 0.76    0.34 0.44\n",
              "capacity and performance       0.88    0.54 0.64\n",
              "reliability                    0.30    0.08 0.13\n",
              "security                       0.81    0.55 0.65\n",
              "usability                      0.90    0.38 0.52\n",
              "other nonfunctional            0.40    0.11 0.17\n",
              "functional                     0.88    0.85 0.87\n",
              "not applicable                 0.92    0.93 0.92\n",
              "MICRO AVG                      0.77    0.45 0.55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-0oM1GGSAs"
      },
      "source": [
        "### Exportamos los modelos\n",
        "Los modelos que exportaremos coincidiran con los entrenados en la primera\n",
        "iteración de la validación cruzada. De esta manera podremos utilizar estos modelos en otro lugar para poder clasificar nuevos elementos. También debemos exportar el modelo (TfidfVectorizer) utilizado para convertir las frases en vectores de números."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k77baK6Iz1a"
      },
      "source": [
        "# Dividimos en datos de entrenamiento y validación\n",
        "train = db.drop(range(0, math.floor(len(db)/k)))\n",
        "test = db[0:math.floor(len(db)/k)]\n",
        "\n",
        "# Instanciamos Vectorizador TFIDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Entrenamos con los elementos de entrenamiento\n",
        "vectorizer.fit(list(train['sentences'].values))\n",
        "# Exportamos el vectorizador\n",
        "dump(vectorizer, './drive/My Drive/nfr-extraction/models/vectorizer.joblib')\n",
        "\n",
        "# Obtenemos X e Y para conjunto de entrenamiento y de validación\n",
        "x_train = vectorizer.transform(list(train['sentences'].values))\n",
        "y_train = train.drop(labels=['sentences'], axis=1)\n",
        "x_test = vectorizer.transform(list(test['sentences'].values))\n",
        "y_test = test.drop(labels=['sentences'], axis=1)\n",
        "\n",
        "# Para cada categoría\n",
        "for category in categories[:-1]:\n",
        "    model = models[model_picker.value]\n",
        "    model.fit(x_train, y_train[category])\n",
        "    dump(model, './drive/My Drive/nfr-extraction/models/' + category.replace(\" \", \"\") + '.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "zvjGe9iLJFjZ",
        "outputId": "ab6984e7-e882-481c-ce59-f0d93212728d"
      },
      "source": [
        "# Creamos diccionario con las traducciones para cada categoría\n",
        "cat_translations = {\n",
        "    \"access control\": \"Control de acceso\", \n",
        "    \"audit\": \"Auditoría\",\n",
        "    \"availability\": \"Disponibilidad\",\n",
        "    \"legal\": \"Legal\",\n",
        "    \"look and feel\": \"Diseño\",\n",
        "    \"maintainability\": \"Mantenibilidad\",\n",
        "    \"operational\": \"Operacional\",\n",
        "    \"privacy\": \"Privacidad\",\n",
        "    \"recoverability\": \"Recuperabilidad\",\n",
        "    \"capacity and performance\": \"Rendimiento\",\n",
        "    \"reliability\": \"Fiabilidad\",\n",
        "    \"security\": \"Seguridad\",\n",
        "    \"usability\": \"Usabilidad\",\n",
        "    \"other nonfunctional\": \"Otros no funcionales\",\n",
        "    \"functional\": \"Funcional\",\n",
        "    \"not applicable\": \"No aplicable\"\n",
        "}\n",
        "\n",
        "elements = x_test.toarray()[1:10]\n",
        "for element in elements:\n",
        "    requirements = []\n",
        "    # Pasamos la frase por cada clasificador (1 por categoría)\n",
        "    for i in range(len(categories)):    \n",
        "        requirements.extend(models[i].predict([element]))\n",
        "\n",
        "    prediction = [category for indx, category in enumerate(categories) if requirements[indx]]\n",
        "    \n",
        "    if prediction == []:\n",
        "        prediction = ['not applicable']\n",
        "\n",
        "    # Mostramos categorías identificadas\n",
        "    print(\"\\nCategorías identificadas: \", end=\"\")\n",
        "    for predict in prediction:\n",
        "        print(cat_translations[predict], end=\", \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-0682ffa21377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Pasamos la frase por cada clasificador (1 por categoría)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrequirements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \"\"\"\n\u001b[0;32m--> 584\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu2NY66cH0N8"
      },
      "source": [
        "db['sentences'][0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}