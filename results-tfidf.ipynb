{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "results-tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d30eb2fe1af344579e5263593b618eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "SVM núcleo lineal",
              "K-NN con k=1",
              "Clasificador bayesiano ingenuo",
              "Bosque aleatorio"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_a00ca4701fdd4f8faeb5031f39194eea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f58187b242fe41498956910b4c9c8fb9"
          }
        },
        "a00ca4701fdd4f8faeb5031f39194eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f58187b242fe41498956910b4c9c8fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rsQKXx3-HtU"
      },
      "source": [
        "# Resultados TF-IDF\n",
        "En este notebook nos encargamos de entrenar los modelos y mostrar resultados para cada uno de ellos. Mostraremos precisión, sensibilidad y F1 para cada categoría así como la media que estos ofrecen. Como técnica de conversión a formato numérico utilizaremos TF-IDF.\n",
        "\n",
        "### Activamos Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECe6jSgYIXaw",
        "outputId": "2545f3bf-4a9e-4d8f-d9ba-0c42cc5e1394"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwHPcePG__B"
      },
      "source": [
        "### Importamos librerías necesarias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLdx4qGNpt4M",
        "outputId": "f5d8f46a-c5fd-431b-b543-30cbdfef04f6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import warnings\n",
        "import ipywidgets as widgets\n",
        "from joblib import dump\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm, naive_bayes\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from scipy.stats import hmean\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxlDaxe35Dr5"
      },
      "source": [
        "### Definimos función que nos dice si una palabra es un adjetivo, un nombre, un verbo o un advervio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNU5Ovw75Kpg"
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn-uQDOuQTfB"
      },
      "source": [
        "### Definimos función que recibe una cadena de texto y devuelve el texto procesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXmtBMfqPKh"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    # Convertimos a minúsculas\n",
        "    new_text = sentence.lower()\n",
        "    \n",
        "    # Eliminamos puntuación\n",
        "    new_text = re.sub(r'[^\\w\\s]', '', new_text)\n",
        "\n",
        "    # Dividimos en tokens\n",
        "    tokens = nltk.tokenize.TreebankWordTokenizer().tokenize(new_text)\n",
        "\n",
        "    # Eliminamos stopwords\n",
        "    tokens = [word for word in tokens if not word in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "    # Stemming\n",
        "    # stemmer = nltk.stem.PorterStemmer()\n",
        "    # new_text = ' '.join([stemmer.stem(w) for w in tokens])\n",
        "    \n",
        "    # lemma\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    new_text = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens])\n",
        "\n",
        "    # Reemplazamos números por #s\n",
        "    if bool(re.search(r'\\d', new_text)):\n",
        "        new_text = re.sub('[0-9]{5,}', '#####', new_text)\n",
        "        new_text = re.sub('[0-9]{4}', '####', new_text)\n",
        "        new_text = re.sub('[0-9]{3}', '###', new_text)\n",
        "        new_text = re.sub('[0-9]{2}', '##', new_text)\n",
        "        # Cuando existe un solo número lo eliminamos\n",
        "        new_text = re.sub('[0-9]{1}', '', new_text)\n",
        "\n",
        "    return new_text"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvQN0LP_7kwN"
      },
      "source": [
        "###Definimos función que calcula la media armónica de 2 números"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLSxDwVA2ZQn"
      },
      "source": [
        "def calculate_f1_score(precision, recall):\r\n",
        "    return 2*((precision * recall) / (precision + recall))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IVNfQm16AHl"
      },
      "source": [
        "### Seleccionamos el modelo que queremos utilizar. Por defecto se utilizará SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d30eb2fe1af344579e5263593b618eaf",
            "a00ca4701fdd4f8faeb5031f39194eea",
            "f58187b242fe41498956910b4c9c8fb9"
          ]
        },
        "id": "6e-rg-Od6IKJ",
        "outputId": "27ffba81-6a4d-43e6-c096-a329740034b9"
      },
      "source": [
        "model_list = [\n",
        "    ('SVM núcleo lineal', 0), \n",
        "    ('K-NN con k=1', 1), \n",
        "    ('Clasificador bayesiano ingenuo', 2), \n",
        "    ('Bosque aleatorio', 3)\n",
        "]\n",
        "model_picker = widgets.Dropdown(options=model_list)\n",
        "print(\"Selecciona un modelo: \")\n",
        "model_picker"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecciona un modelo: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d30eb2fe1af344579e5263593b618eaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Dropdown(options=(('SVM núcleo lineal', 0), ('K-NN con k=1', 1), ('Clasificador bayesiano ingenuo', 2), ('Bosq…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7vQn1pQA_Xx"
      },
      "source": [
        "### Cargamos y preprocesamos la base de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OPFzy0jBAj5"
      },
      "source": [
        "# Cargamos base de datos\n",
        "RUTA_DB = \"./drive/My Drive/nfr-extraction/db.xlsx\"\n",
        "db = pd.read_excel(RUTA_DB)\n",
        "\n",
        "# Definimos categorías existentes\n",
        "categories = [\n",
        "    \"access control\", \"audit\", \"availability\", \"legal\", \"look and feel\",\n",
        "    \"maintainability\", \"operational\", \"privacy\", \"recoverability\", \"capacity and performance\",\n",
        "    \"reliability\", \"security\", \"usability\", \"other nonfunctional\", \"functional\", \"not applicable\"\n",
        "]\n",
        "# Creamos diccionario con las traducciones para cada categoría\n",
        "cat_translations =[\n",
        "    \"Control de acceso\", \"Auditoría\", \"Disponibilidad\", \"Legal\", \"Diseño\",\n",
        "    \"Mantenibilidad\", \"Operacional\", \"Privacidad\", \"Recuperabilidad\", \"Rendimiento\",\n",
        "    \"Fiabilidad\", \"Seguridad\", \"Usabilidad\", \"Otros no funcionales\", \"Funcional\", \"No aplicable\"    \n",
        "]\n",
        "\n",
        "# Preprocesamos los elementos de la base de datos y barajamos de forma aleatoria\n",
        "db['sentences'] = db['sentences'].apply(preprocess)\n",
        "db = db.dropna()\n",
        "\n",
        "db = db.sample(frac=1, random_state = 42).reset_index(drop=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToMTS1n9Qn1m"
      },
      "source": [
        "### Entrenamos modelo, validamos y mostramos resultados de precisión, sensibilidad y F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "wIber3M3vASL",
        "outputId": "c52afb71-4254-411a-8bc7-7234be9fd0fd"
      },
      "source": [
        "# Suprimimos posibles alertas\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "# Definimos lista con los posibles modelos\n",
        "models = [svm.LinearSVC(), KNeighborsClassifier(n_neighbors=1), naive_bayes.MultinomialNB(), RandomForestClassifier(n_jobs=-1)]\n",
        "# Escogemos el modelo que vamos a utilizar\n",
        "model = models[model_picker.value]\n",
        "\n",
        "# Definimos variables necesarias para el almacenamiento de los resultados\n",
        "n_splits = 10\n",
        "mean_precision = [[0]*16]*n_splits\n",
        "mean_recall = [[0]*16]*n_splits\n",
        "mean_f1score = [[0]*16]*n_splits\n",
        "i_test = 0 \n",
        "f_test = math.floor(len(db)/n_splits)\n",
        "\n",
        "# Iteramos n_splits veces (Validación cruzada con n_splits = 10)\n",
        "for i_split in range(n_splits):\n",
        "    print(\"\\nIteración\", i_split+1, end='')\n",
        "    # Definimos variables de precisión, sensibilidad y F1 para esta iteración \n",
        "    precision = [0]*len(categories)\n",
        "    recall = [0]*len(categories)\n",
        "    f1score = [0]*len(categories)\n",
        "\n",
        "    # Dividimos en datos de entrenamiento y validación\n",
        "    train = db.drop(range(i_test, f_test))\n",
        "    test = db[i_test:f_test]\n",
        "\n",
        "    # Instanciamos el modelo que se encargará de aplicar tfidf a las frases\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Lo entrenamos con el conjunto de datos de entrenamiento\n",
        "    vectorizer.fit(list(train['sentences'].values))\n",
        "\n",
        "    # Dividimos en vectores de características de entrenamiento y validación,\n",
        "    # y clasificaciones para entrenamiento y validaciób\n",
        "    x_train = normalize(vectorizer.transform(list(train['sentences'].values)))\n",
        "    y_train = train.drop(labels=['sentences'], axis=1)\n",
        "    x_test = normalize(vectorizer.transform(list(test['sentences'].values)))\n",
        "    y_test = test.drop(labels=['sentences'], axis=1)\n",
        "\n",
        "    # Para cada categoría\n",
        "    for i, category in enumerate(categories):\n",
        "        print('.', end='')\n",
        "        # Entrenamos el modelo\n",
        "        model.fit(x_train, y_train[category])\n",
        "        # Realizamos predicción para conjunto de prueba\n",
        "        prediction = model.predict(x_test)\n",
        "        # Obtenemos resultados\n",
        "        precision[i] = precision_score(list(y_test[category].values), prediction)\n",
        "        recall[i] = recall_score(list(y_test[category].values), prediction)\n",
        "    \n",
        "    i_test = f_test\n",
        "    f_test = i_test + math.floor(len(db)/n_splits)\n",
        "    \n",
        "    mean_precision[i_split] = precision\n",
        "    mean_recall[i_split] = recall\n",
        "    mean_f1score[i_split] = f1score\n",
        "\n",
        "# Mostramos resultados agrupados en un dataframe de pandas\n",
        "p = list(np.average(mean_precision, axis=0))\n",
        "rounded_p = [round(elem, 2) for elem in p]\n",
        "r = list(np.average(mean_recall, axis=0))\n",
        "rounded_r = [round(elem, 2) for elem in r]\n",
        "rounded_f1 = [\n",
        "    round(calculate_f1_score(rounded_p[i], rounded_r[i]), 2) for i, elem in enumerate(rounded_p)\n",
        "]\n",
        "data = {\n",
        "    \" \": cat_translations + ['MICRO AVG'],\n",
        "    \"Precisión\": rounded_p + [np.average(rounded_p)],\n",
        "    \"Sensibilidad\": rounded_r + [np.average(rounded_r)],\n",
        "    \"F1\": rounded_f1 + [calculate_f1_score(np.average(rounded_p), np.average(rounded_r))]\n",
        "}\n",
        "print(\"\\nLos resultados para el modelo seleccionado son:\")\n",
        "pd.options.display.float_format = \"{:,.2f}\".format\n",
        "pd.DataFrame(data).set_index(' ')\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteración 1................\n",
            "Iteración 2................\n",
            "Iteración 3................\n",
            "Iteración 4................\n",
            "Iteración 5................\n",
            "Iteración 6................\n",
            "Iteración 7................\n",
            "Iteración 8................\n",
            "Iteración 9................\n",
            "Iteración 10................\n",
            "Los resultados para el modelo seleccionado son:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Precisión</th>\n",
              "      <th>Sensibilidad</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Control de acceso</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Auditoría</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Disponibilidad</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Legal</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Diseño</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mantenibilidad</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Operacional</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Privacidad</th>\n",
              "      <td>0.79</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recuperabilidad</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rendimiento</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fiabilidad</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Seguridad</th>\n",
              "      <td>0.79</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Usabilidad</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Otros no funcionales</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Funcional</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>No aplicable</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MICRO AVG</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Precisión  Sensibilidad   F1\n",
              "                                                  \n",
              "Control de acceso          0.80          0.64 0.71\n",
              "Auditoría                  0.80          0.46 0.58\n",
              "Disponibilidad             0.68          0.35 0.46\n",
              "Legal                      0.87          0.46 0.60\n",
              "Diseño                     0.80          0.36 0.50\n",
              "Mantenibilidad             0.84          0.46 0.59\n",
              "Operacional                0.70          0.37 0.48\n",
              "Privacidad                 0.79          0.46 0.58\n",
              "Recuperabilidad            0.76          0.37 0.50\n",
              "Rendimiento                0.87          0.55 0.67\n",
              "Fiabilidad                 0.20          0.06 0.09\n",
              "Seguridad                  0.79          0.53 0.63\n",
              "Usabilidad                 0.89          0.37 0.52\n",
              "Otros no funcionales       0.40          0.10 0.16\n",
              "Funcional                  0.88          0.84 0.86\n",
              "No aplicable               0.91          0.92 0.91\n",
              "MICRO AVG                  0.75          0.46 0.57"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-0oM1GGSAs"
      },
      "source": [
        "### Exportamos los modelos\n",
        "Los modelos que exportaremos coincidiran con los entrenados en la primera\n",
        "iteración de la validación cruzada. De esta manera podremos utilizar estos modelos en otro lugar para poder clasificar nuevos elementos. También debemos exportar el modelo (TfidfVectorizer) utilizado para convertir las frases en vectores de números."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k77baK6Iz1a"
      },
      "source": [
        "# Dividimos en datos de entrenamiento y validación\n",
        "train = db.drop(range(0, math.floor(len(db)/n_splits)))\n",
        "test = db[0:math.floor(len(db)/n_splits)]\n",
        "\n",
        "# Instanciamos Vectorizador TFIDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Entrenamos con los elementos de entrenamiento\n",
        "vectorizer.fit(list(train['sentences'].values))\n",
        "# Exportamos el vectorizador\n",
        "dump(vectorizer, './drive/My Drive/nfr-extraction/models-tfidf/vectorizer.joblib')\n",
        "\n",
        "# Obtenemos X e Y para conjunto de entrenamiento y de validación\n",
        "x_train = vectorizer.transform(list(train['sentences'].values))\n",
        "y_train = train.drop(labels=['sentences'], axis=1)\n",
        "x_test = vectorizer.transform(list(test['sentences'].values))\n",
        "y_test = test.drop(labels=['sentences'], axis=1)\n",
        "\n",
        "# Para cada categoría\n",
        "for category in categories[:-1]:\n",
        "    model = models[model_picker.value]\n",
        "    model.fit(x_train, y_train[category])\n",
        "    dump(model, './drive/My Drive/nfr-extraction/models-tfidf/' + category.replace(\" \", \"\") + '.joblib')\n"
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}